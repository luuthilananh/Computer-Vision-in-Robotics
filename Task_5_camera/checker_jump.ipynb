{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "629a088d-0c70-4386-8d7d-185ec215d05a",
   "metadata": {},
   "source": [
    "## Checkers Jump\n",
    "1. detect two objects of different colors in the camera frame.\n",
    "2. from seminar_6 we get our camera intrinsic matrix A and distortion coefficients (dis) \n",
    "3. Calculate the position the first object must move to in order to \"capture\" the second object (i.e., jump over it), \n",
    "and draw the target position on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a50521-381f-4207-9e51-b01d6d98d7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# detect two objects of different colors in HSV\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Callback function (does nothing)\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Create a window\n",
    "cv2.namedWindow(\"mask\")\n",
    "\n",
    "# Create trackbars to adjust the HSV threshold values\n",
    "cv2.createTrackbar(\"low_H\", \"mask\", 0, 255, nothing)  # Hue (lower limit)\n",
    "cv2.createTrackbar(\"low_S\", \"mask\", 0, 255, nothing)  # Saturation (lower limit)\n",
    "cv2.createTrackbar(\"low_V\", \"mask\", 0, 255, nothing)  # Value (lower limit)\n",
    "cv2.createTrackbar(\"high_H\", \"mask\", 155, 255, nothing)  # Hue (upper limit)\n",
    "cv2.createTrackbar(\"high_S\", \"mask\", 155, 255, nothing)  # Saturation (upper limit)\n",
    "cv2.createTrackbar(\"high_V\", \"mask\", 155, 255, nothing)  # Value (upper limit)\n",
    "\n",
    "while True:\n",
    "    # Capture a frame from the webcam\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Convert the frame from BGR to HSVя\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Get the trackbar values for HSV range\n",
    "    low_H = cv2.getTrackbarPos(\"low_H\", \"mask\")\n",
    "    low_S = cv2.getTrackbarPos(\"low_S\", \"mask\")\n",
    "    low_V = cv2.getTrackbarPos(\"low_V\", \"mask\")\n",
    "    high_H = cv2.getTrackbarPos(\"high_H\", \"mask\")\n",
    "    high_S = cv2.getTrackbarPos(\"high_S\", \"mask\")\n",
    "    high_V = cv2.getTrackbarPos(\"high_V\", \"mask\")\n",
    "\n",
    "    # Create the lower and upper bounds for the HSV range\n",
    "    lower_bound = np.array([low_H, low_S, low_V])\n",
    "    upper_bound = np.array([high_H, high_S, high_V])\n",
    "\n",
    "    # Create the mask to filter the colors within the specified HSV range\n",
    "    mask = cv2.inRange(hsv, lower_bound, upper_bound)\n",
    "\n",
    "    # Display the original, mask, and filtered result\n",
    "    cv2.imshow(\"Original\", frame)\n",
    "    cv2.imshow(\"mask\", mask)\n",
    "\n",
    "    # Press 'q' to exit the loop\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a4bf11d-0f03-41a7-99c8-a9df96b9dfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculate the position the first object must move to in order to \"capture\" the second object (i.e., jump over it), \n",
    "and draw the target position on the image.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "# Camera intrinsic matrix\n",
    "A = np.array([[653.79017244, 0.00000000, 325.45730598],\n",
    "              [0.00000000, 654.02497422, 240.77670025],\n",
    "              [0.00000000, 0.00000000, 1.00000000]])\n",
    "\n",
    "# Camera distortion coefficients\n",
    "dist = np.array([[-0.19878638,  0.50415653,  0.00260482,  0.00095636,  -0.52617064]])\n",
    "\n",
    "# Function to convert image coordinates to real-world coordinates\n",
    "def pic2r(A, alpha, beta, x, y, h):\n",
    "    fx = A[0, 0]\n",
    "    fy = A[1, 1]\n",
    "    cx = A[0, 2]\n",
    "    cy = A[1, 2]\n",
    "\n",
    "    Xc = (x - cx) / fx\n",
    "    Yc = (y - cy) / fy\n",
    "\n",
    "    Zr = h / math.tan(beta)\n",
    "\n",
    "    Xr = Zr * Xc\n",
    "    Yr = Zr * Yc\n",
    "\n",
    "    return Xr, Yr\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    success, frame = cam.read()\n",
    "    if not success:\n",
    "        print(\"Cannot read frame. Exiting\")\n",
    "        break\n",
    "\n",
    "     # Apply blurring and convert frame to HSV color space\n",
    "    blurred = cv2.blur(frame, (7, 7))\n",
    "    hsv = cv2.cvtColor(blurred, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    #  Detect red object\n",
    "    mask1 = cv2.inRange(hsv, (0, 150, 143), (219, 255, 255)) # first obj - red\n",
    "    mask2 = cv2.inRange(hsv, (48, 117, 16), (92, 255, 166))  # second obj - green\n",
    "\n",
    "    # Combine both masks\n",
    "    combined_mask = cv2.bitwise_or(mask1, mask2)\n",
    "    cv2.imshow(\"Mask\", combined_mask)\n",
    "\n",
    "    # Function to detect objects based on color mask\n",
    "    def detect_object(mask, color):\n",
    "        connectivity = 4\n",
    "        output = cv2.connectedComponentsWithStats(mask, connectivity, cv2.CV_32S)\n",
    "        \n",
    "        num_labels = output[0]\n",
    "        stats = output[2]\n",
    "        objects = []\n",
    "        \n",
    "        for i in range(1, num_labels):\n",
    "            a = stats[i, cv2.CC_STAT_AREA]\n",
    "            t = stats[i, cv2.CC_STAT_TOP]\n",
    "            l = stats[i, cv2.CC_STAT_LEFT]\n",
    "            w = stats[i, cv2.CC_STAT_WIDTH]\n",
    "            h = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "            \n",
    "            if a >= 1500:\n",
    "                cx = l + w // 2  \n",
    "                cy = t + h // 2  \n",
    "                objects.append((cx, cy))\n",
    "                cv2.rectangle(frame, (l, t), (l + w, t + h), color, 3)\n",
    "        \n",
    "        return objects\n",
    "\n",
    "    objects1 = detect_object(mask1, (0, 0, 0))   # Màu đỏ\n",
    "    objects2 = detect_object(mask2, (0, 0, 0))   # Màu xanh lá\n",
    "\n",
    "    # Display object positions\n",
    "    if objects1:\n",
    "        obj1 = objects1[0]  \n",
    "        cv2.putText(frame, \"Obj1\", (obj1[0] - 20, obj1[1] - 20), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    if objects2:\n",
    "        obj2 = objects2[0]  \n",
    "        cv2.putText(frame, \"Obj2\", (obj2[0] - 20, obj2[1] - 20), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        \n",
    "    # Calculate target position if both objects are detected\n",
    "    if objects1 and objects2:\n",
    "        obj1 = objects1[0]  \n",
    "        obj2 = objects2[0]  \n",
    "        \n",
    "        target_x = 2 * obj2[0] - obj1[0] \n",
    "        target_y = 2 * obj2[1] - obj1[1]\n",
    "        \n",
    "        cv2.circle(frame, (target_x, target_y), 10, (255, 0, 0), -1)\n",
    "        \n",
    "        cv2.putText(frame, \"TARGET\", (target_x - 40, target_y - 40), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    1, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "        '''\n",
    "        (target_x - 40, target_y - 40) → The position (x, y) where the text will appear - moves the text slightly left and upward \n",
    "        1 → Font scale (the size of the text); (0, 0, 255) → The color of the text in BGR format; 1 → Thickness of the text stroke.\n",
    "\n",
    "        '''\n",
    "        # Convert to real-world coordinates\n",
    "        xr, yr = pic2r(A, 0, (90 - 54) * math.pi / 180, target_x, target_y, 30)  # beta = (90 - 54) * math.pi / 180 () represents the angle relative to the horizontal plane.\n",
    "        cv2.putText(frame, f\"Real: ({xr:.2f}, {yr:.2f})\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 3)\n",
    "    \n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "    key = cv2.waitKey(30) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ecf1d3-4e59-436d-bc78-e067b657f9ef",
   "metadata": {},
   "source": [
    "## Key point\n",
    "implement object search (keypoints) from template(detail) on video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d7e1f38-e7be-4e36-8e08-d619d4c16490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The code still fails to 'accurately identify matching keypoints between the reference image and the current frame.\n",
    "As observed, many keypoints do not belong to the object (these from the hand) but are mistakenly identified. Is there a way to optimize this further?\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Path to reference image and video file\n",
    "ref_path = \"ref.jpg\"\n",
    "vid_path = \"vid.mp4\"\n",
    "\n",
    "reference = cv2.imread(ref_path, cv2.IMREAD_GRAYSCALE)\n",
    "cam = cv2.VideoCapture(vid_path)\n",
    "\n",
    "# Инициализация SIFT , # Initialize SIFT (Scale-Invariant Feature Transform) detector\n",
    "sift = cv2.SIFT_create()\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(reference, None)\n",
    "\n",
    "# FLANN-поиск # FLANN (Fast Library for Approximate Nearest Neighbors) matcher settings\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=100)  # Number of checks for nearest neighbors\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    \n",
    "    if not ret:\n",
    "        cam.release()\n",
    "        cam = cv2.VideoCapture(vid_path)\n",
    "        continue\n",
    "    \n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    keypoints2, descriptors2 = sift.detectAndCompute(gray_frame, None)\n",
    "\n",
    "    # Perform feature matching only if there are valid descriptors\n",
    "    if descriptors2 is not None and len(descriptors2) > 0:\n",
    "        matches = flann.knnMatch(descriptors1, descriptors2, k=2)\n",
    "        \n",
    "        good_matches = []\n",
    "        for m, n in matches:\n",
    "            if m.distance < 0.65 * n.distance:\n",
    "                good_matches.append(m)\n",
    "        \n",
    "        result = cv2.drawMatches(reference, keypoints1, frame, keypoints2, good_matches[:30], None,\n",
    "                                 flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "        \n",
    "        result_resized = cv2.resize(result, (1000, 900))\n",
    "    else:\n",
    "        result = frame\n",
    "    \n",
    "    cv2.imshow(\"matched\", result_resized)\n",
    "    \n",
    "    key = cv2.waitKey(5000) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff02011-65ef-42b8-8709-414664e3ae9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
